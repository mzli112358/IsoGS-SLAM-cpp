import argparse
import json
import os
import shutil
import sys
import time
import csv
from importlib.machinery import SourceFileLoader

_BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

sys.path.insert(0, _BASE_DIR)

print("System Paths:")
for p in sys.path:
    print(p)

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
import wandb

from datasets.gradslam_datasets import (load_dataset_config, ICLDataset, ReplicaDataset, ReplicaV2Dataset, AzureKinectDataset,
                                        ScannetDataset, Ai2thorDataset, Record3DDataset, RealsenseDataset, TUMDataset,
                                        ScannetPPDataset, NeRFCaptureDataset)
from utils.common_utils import seed_everything, save_params_ckpt, save_params
from utils.eval_helpers import report_loss, report_progress, eval
from utils.keyframe_selection import keyframe_selection_overlap
from utils.recon_helpers import setup_camera
from utils.slam_helpers import (
    transformed_params2rendervar, transformed_params2depthplussilhouette,
    transform_to_frame, l1_loss_v1, matrix_to_quaternion
)
from utils.slam_external import calc_ssim, build_rotation, prune_gaussians, densify

from diff_gaussian_rasterization import GaussianRasterizer as Renderer


# [IsoGS] -------------------------------------------
# Core computation functions for geometric constraints (extracted for optimization)
def compute_flat_loss(scales):
    """
    Compute flatness loss from Gaussian scales.
    
    Args:
        scales: [N, 3] tensor of Gaussian scales
    
    Returns:
        loss_flat: scalar tensor
    """
    # Compute the minimum axis (smallest scale) for each Gaussian and take mean
    loss_flat = torch.mean(torch.min(scales, dim=1).values)
    return loss_flat


# [IsoGS] -------------------------------------------
# CSV 日志相关的辅助函数，用于记录每帧 Tracking / Mapping 指标，支持断点续跑时覆盖后续帧
def _init_metrics_csv(output_dir, checkpoint_time_idx):
    """
    初始化（或截断）指标 CSV 文件。
    
    规则：
    - 文件名固定为 metrics_log.csv，位于当前 run 目录下；
    - 如果不存在则创建并写入表头；
    - 如果已存在且 checkpoint_time_idx > 0，则只保留 frame < checkpoint_time_idx 的行，
      这样从 checkpoint 继续运行时，会覆盖 checkpoint 之后的帧的记录。
    """
    csv_path = os.path.join(output_dir, "metrics_log.csv")
    fieldnames = [
        "frame",          # 当前全局帧号 time_idx
        "stage",          # 'tracking' 或 'mapping'
        "step",           # 对应的迭代 step（与终端打印中的 Step 一致）
        "loss",
        "image_loss",
        "depth_loss",
        "flat_loss",
        "iso_loss",
        "mean_density",
    ]

    # 如果文件不存在，直接创建空表头
    if not os.path.exists(csv_path):
        with open(csv_path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
        return csv_path, fieldnames

    # 文件存在的情况：根据 checkpoint_time_idx 截断
    keep_rows = []
    try:
        with open(csv_path, "r", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    frame_val = int(row.get("frame", -1))
                except ValueError:
                    # 非法行直接丢弃
                    continue
                # 只保留 checkpoint 之前的帧，后面的会被本次运行覆盖
                if checkpoint_time_idx is not None and frame_val >= 0 and frame_val < checkpoint_time_idx:
                    keep_rows.append(row)
    except Exception:
        # 读失败时，直接重新创建文件，避免中断主流程
        keep_rows = []

    # 重写文件：写入表头 + 保留的旧记录
    with open(csv_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        if keep_rows:
            writer.writerows(keep_rows)

    return csv_path, fieldnames


def _append_metrics_row(csv_path, fieldnames, frame_idx, stage, step_idx, losses):
    """
    追加一行指标到 CSV：
    - frame_idx: 当前全局帧编号（time_idx）
    - stage: 'tracking' 或 'mapping'
    - step_idx: 与终端中 [Tracking]/[Mapping] Step 对应的迭代编号
    - losses: get_loss / report_loss 中的 losses 字典
    """
    if csv_path is None:
        return

    def _to_float(val, default=0.0):
        try:
            if val is None:
                return default
            # 兼容 torch.Tensor
            if hasattr(val, "item"):
                return float(val.item())
            return float(val)
        except Exception:
            return default

    row = {
        "frame": int(frame_idx),
        "stage": str(stage),
        "step": int(step_idx),
        "loss": _to_float(losses.get("loss")),
        "image_loss": _to_float(losses.get("im")),
        "depth_loss": _to_float(losses.get("depth")),
        "flat_loss": _to_float(losses.get("flat")),
        "iso_loss": _to_float(losses.get("iso")),
        "mean_density": _to_float(losses.get("mean_density"), default=0.0),
    }

    try:
        with open(csv_path, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writerow(row)
    except Exception:
        # 日志失败不应影响主流程，静默忽略
        pass


def compute_iso_surface_loss_sampled(
    query_points,  # [sample_size, 3]
    means,  # [N, 3]
    inverse_covariances,  # [N, 3, 3]
    opacities,  # [N, 1]
    K,  # int
    target_saturation,  # float
    chunk_size=128,  # int: internal batch size to avoid OOM
):
    """
    Compute iso-surface density loss using sampled query points with internal batching.
    
    This function performs KNN search and density computation for sampled points only.
    Uses internal chunking to avoid OOM when dealing with large numbers of Gaussians.
    
    Args:
        query_points: [sample_size, 3] sampled query points
        means: [N, 3] all Gaussian means
        inverse_covariances: [N, 3, 3] all inverse covariance matrices
        opacities: [N, 1] all Gaussian opacities
        K: number of nearest neighbors
        target_saturation: target density value
        chunk_size: internal batch size for processing query points (default: 1024)
    
    Returns:
        loss_iso: scalar tensor
        density_val: [sample_size] density values for monitoring
    """
    sample_size = query_points.shape[0]
    num_gaussians = means.shape[0]
    
    # [IsoGS] Internal batching: process query_points in chunks to avoid OOM
    # Accumulate density values and loss contributions across chunks
    all_density_vals = []
    all_loss_contributions = []
    
    num_chunks = (sample_size + chunk_size - 1) // chunk_size  # Ceiling division
    
    for chunk_idx in range(num_chunks):
        start_idx = chunk_idx * chunk_size
        end_idx = min(start_idx + chunk_size, sample_size)
        chunk_query_points = query_points[start_idx:end_idx]  # [chunk_size, 3]
        chunk_size_actual = chunk_query_points.shape[0]
        
        # KNN search using torch.cdist for this chunk
        # Compute distance matrix: [chunk_size_actual, N]
        chunk_distances = torch.cdist(chunk_query_points, means)  # [chunk_size_actual, num_gaussians]
        # Find K nearest neighbors
        _, chunk_neighbor_indices = torch.topk(chunk_distances, k=min(K, num_gaussians), dim=1, largest=False)  # [chunk_size_actual, K]
        
        # Collect neighbor data for this chunk
        # Expand chunk_query_points for broadcasting: [chunk_size_actual, 1, 3]
        chunk_query_points_expanded = chunk_query_points.unsqueeze(1)  # [chunk_size_actual, 1, 3]
        
        # Gather neighbor means: [chunk_size_actual, K, 3]
        chunk_neighbor_means = means[chunk_neighbor_indices]  # [chunk_size_actual, K, 3]
        
        # Compute delta vectors: [chunk_size_actual, K, 3]
        chunk_deltas = chunk_query_points_expanded - chunk_neighbor_means  # [chunk_size_actual, K, 3]
        
        # Gather neighbor inverse covariances: [chunk_size_actual, K, 3, 3]
        chunk_neighbor_inv_covs = inverse_covariances[chunk_neighbor_indices]  # [chunk_size_actual, K, 3, 3]
        
        # Gather neighbor opacities: [chunk_size_actual, K, 1]
        chunk_neighbor_opacities = opacities[chunk_neighbor_indices]  # [chunk_size_actual, K, 1]
        
        # Compute density: D = sum(α_j * exp(-0.5 * Δ^T * Σ_j^{-1} * Δ))
        # Reshape for batch matrix multiplication
        chunk_deltas_reshaped = chunk_deltas.unsqueeze(-1)  # [chunk_size_actual, K, 3, 1]
        
        # Compute inv_cov @ delta: [chunk_size_actual, K, 3, 3] @ [chunk_size_actual, K, 3, 1] = [chunk_size_actual, K, 3, 1]
        chunk_inv_cov_delta = torch.matmul(chunk_neighbor_inv_covs, chunk_deltas_reshaped)  # [chunk_size_actual, K, 3, 1]
        
        # Compute delta^T @ (inv_cov @ delta): [chunk_size_actual, K, 1, 3] @ [chunk_size_actual, K, 3, 1] = [chunk_size_actual, K, 1, 1]
        chunk_deltas_T = chunk_deltas.unsqueeze(-2)  # [chunk_size_actual, K, 1, 3]
        chunk_quad_form = torch.matmul(chunk_deltas_T, chunk_inv_cov_delta).squeeze(-1).squeeze(-1)  # [chunk_size_actual, K]
        
        # Compute exponential: exp(-0.5 * quad_form)
        chunk_exp_term = torch.exp(-0.5 * chunk_quad_form)  # [chunk_size_actual, K]
        
        # Multiply by opacities and sum over neighbors
        chunk_neighbor_opacities_squeezed = chunk_neighbor_opacities.squeeze(-1)  # [chunk_size_actual, K]
        chunk_density_per_neighbor = chunk_neighbor_opacities_squeezed * chunk_exp_term  # [chunk_size_actual, K]
        chunk_density_sum = chunk_density_per_neighbor.sum(dim=1)  # [chunk_size_actual]
        
        # Store density values for this chunk
        all_density_vals.append(chunk_density_sum)
        
        # Compute loss contribution for this chunk
        chunk_loss_contribution = (chunk_density_sum - target_saturation) ** 2
        all_loss_contributions.append(chunk_loss_contribution)
    
    # Concatenate all density values
    density_val = torch.cat(all_density_vals, dim=0)  # [sample_size]
    
    # Compute overall loss as mean of all contributions
    loss_iso = torch.mean(torch.cat(all_loss_contributions, dim=0))

    # [IsoGS] Explicitly free large temporary tensors and reclaim fragmented memory
    if "chunk_distances" in locals():
        del chunk_distances
    torch.cuda.empty_cache()

    return loss_iso, density_val


    # [IsoGS] Use pure Python implementations directly (no torch.compile)


def get_dataset(config_dict, basedir, sequence, **kwargs):
    if config_dict["dataset_name"].lower() in ["icl"]:
        return ICLDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["replica"]:
        return ReplicaDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["replicav2"]:
        return ReplicaV2Dataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["azure", "azurekinect"]:
        return AzureKinectDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["scannet"]:
        return ScannetDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["ai2thor"]:
        return Ai2thorDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["record3d"]:
        return Record3DDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["realsense"]:
        return RealsenseDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["tum"]:
        return TUMDataset(config_dict, basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["scannetpp"]:
        return ScannetPPDataset(basedir, sequence, **kwargs)
    elif config_dict["dataset_name"].lower() in ["nerfcapture"]:
        return NeRFCaptureDataset(basedir, sequence, **kwargs)
    else:
        raise ValueError(f"Unknown dataset name {config_dict['dataset_name']}")


def get_pointcloud(color, depth, intrinsics, w2c, transform_pts=True, 
                   mask=None, compute_mean_sq_dist=False, mean_sq_dist_method="projective"):
    width, height = color.shape[2], color.shape[1]
    CX = intrinsics[0][2]
    CY = intrinsics[1][2]
    FX = intrinsics[0][0]
    FY = intrinsics[1][1]

    # Compute indices of pixels
    x_grid, y_grid = torch.meshgrid(torch.arange(width).cuda().float(), 
                                    torch.arange(height).cuda().float(),
                                    indexing='xy')
    xx = (x_grid - CX)/FX
    yy = (y_grid - CY)/FY
    xx = xx.reshape(-1)
    yy = yy.reshape(-1)
    depth_z = depth[0].reshape(-1)

    # Initialize point cloud
    pts_cam = torch.stack((xx * depth_z, yy * depth_z, depth_z), dim=-1)
    if transform_pts:
        pix_ones = torch.ones(height * width, 1).cuda().float()
        pts4 = torch.cat((pts_cam, pix_ones), dim=1)
        c2w = torch.inverse(w2c)
        pts = (c2w @ pts4.T).T[:, :3]
    else:
        pts = pts_cam

    # Compute mean squared distance for initializing the scale of the Gaussians
    if compute_mean_sq_dist:
        if mean_sq_dist_method == "projective":
            # Projective Geometry (this is fast, farther -> larger radius)
            scale_gaussian = depth_z / ((FX + FY)/2)
            mean3_sq_dist = scale_gaussian**2
        else:
            raise ValueError(f"Unknown mean_sq_dist_method {mean_sq_dist_method}")
    
    # Colorize point cloud
    cols = torch.permute(color, (1, 2, 0)).reshape(-1, 3) # (C, H, W) -> (H, W, C) -> (H * W, C)
    point_cld = torch.cat((pts, cols), -1)

    # Select points based on mask
    if mask is not None:
        point_cld = point_cld[mask]
        if compute_mean_sq_dist:
            mean3_sq_dist = mean3_sq_dist[mask]

    if compute_mean_sq_dist:
        return point_cld, mean3_sq_dist
    else:
        return point_cld


def initialize_params(init_pt_cld, num_frames, mean3_sq_dist, gaussian_distribution):
    num_pts = init_pt_cld.shape[0]
    means3D = init_pt_cld[:, :3] # [num_gaussians, 3]
    unnorm_rots = np.tile([1, 0, 0, 0], (num_pts, 1)) # [num_gaussians, 4]
    logit_opacities = torch.zeros((num_pts, 1), dtype=torch.float, device="cuda")
    # [IsoGS] Force 3D (anisotropic) initialization for flatness regularization
    # Even if config says "isotropic", we initialize as 3D with small random perturbation
    base_log_scale = torch.log(torch.sqrt(mean3_sq_dist))[..., None]  # [num_pts, 1]
    if gaussian_distribution == "isotropic":
        # Convert to 3D by tiling and adding small random perturbation to break symmetry
        # This allows flatness regularization to work even with "isotropic" config
        log_scales_base = torch.tile(base_log_scale, (1, 3))  # [num_pts, 3]
        # Add small random perturbation (std=0.01) to prevent identical gradients
        perturbation = torch.randn_like(log_scales_base) * 0.01
        log_scales = log_scales_base + perturbation
        if not hasattr(initialize_params, '_warned_forced_3d'):
            print("[IsoGS] Forced 3D initialization: Converting isotropic to anisotropic for flatness regularization.")
            initialize_params._warned_forced_3d = True
    elif gaussian_distribution == "anisotropic":
        log_scales = torch.tile(base_log_scale, (1, 3))  # [num_pts, 3]
    else:
        raise ValueError(f"Unknown gaussian_distribution {gaussian_distribution}")
    params = {
        'means3D': means3D,
        'rgb_colors': init_pt_cld[:, 3:6],
        'unnorm_rotations': unnorm_rots,
        'logit_opacities': logit_opacities,
        'log_scales': log_scales,
    }

    # Initialize a single gaussian trajectory to model the camera poses relative to the first frame
    cam_rots = np.tile([1, 0, 0, 0], (1, 1))
    cam_rots = np.tile(cam_rots[:, :, None], (1, 1, num_frames))
    params['cam_unnorm_rots'] = cam_rots
    params['cam_trans'] = np.zeros((1, 3, num_frames))

    for k, v in params.items():
        # Check if value is already a torch tensor
        if not isinstance(v, torch.Tensor):
            params[k] = torch.nn.Parameter(torch.tensor(v).cuda().float().contiguous().requires_grad_(True))
        else:
            params[k] = torch.nn.Parameter(v.cuda().float().contiguous().requires_grad_(True))

    variables = {'max_2D_radius': torch.zeros(params['means3D'].shape[0]).cuda().float(),
                 'means2D_gradient_accum': torch.zeros(params['means3D'].shape[0]).cuda().float(),
                 'denom': torch.zeros(params['means3D'].shape[0]).cuda().float(),
                 'timestep': torch.zeros(params['means3D'].shape[0]).cuda().float()}

    return params, variables


def initialize_optimizer(params, lrs_dict, tracking):
    lrs = lrs_dict
    param_groups = [{'params': [v], 'name': k, 'lr': lrs[k]} for k, v in params.items()]
    if tracking:
        return torch.optim.Adam(param_groups)
    else:
        return torch.optim.Adam(param_groups, lr=0.0, eps=1e-15)


def initialize_first_timestep(dataset, num_frames, scene_radius_depth_ratio, 
                              mean_sq_dist_method, densify_dataset=None, gaussian_distribution=None):
    # Get RGB-D Data & Camera Parameters
    color, depth, intrinsics, pose = dataset[0]

    # Process RGB-D Data
    color = color.permute(2, 0, 1) / 255 # (H, W, C) -> (C, H, W)
    depth = depth.permute(2, 0, 1) # (H, W, C) -> (C, H, W)
    
    # Process Camera Parameters
    intrinsics = intrinsics[:3, :3]
    w2c = torch.linalg.inv(pose)

    # Setup Camera
    cam = setup_camera(color.shape[2], color.shape[1], intrinsics.cpu().numpy(), w2c.detach().cpu().numpy())

    if densify_dataset is not None:
        # Get Densification RGB-D Data & Camera Parameters
        color, depth, densify_intrinsics, _ = densify_dataset[0]
        color = color.permute(2, 0, 1) / 255 # (H, W, C) -> (C, H, W)
        depth = depth.permute(2, 0, 1) # (H, W, C) -> (C, H, W)
        densify_intrinsics = densify_intrinsics[:3, :3]
        densify_cam = setup_camera(color.shape[2], color.shape[1], densify_intrinsics.cpu().numpy(), w2c.detach().cpu().numpy())
    else:
        densify_intrinsics = intrinsics

    # Get Initial Point Cloud (PyTorch CUDA Tensor)
    mask = (depth > 0) # Mask out invalid depth values
    mask = mask.reshape(-1)
    init_pt_cld, mean3_sq_dist = get_pointcloud(color, depth, densify_intrinsics, w2c, 
                                                mask=mask, compute_mean_sq_dist=True, 
                                                mean_sq_dist_method=mean_sq_dist_method)

    # Initialize Parameters
    params, variables = initialize_params(init_pt_cld, num_frames, mean3_sq_dist, gaussian_distribution)

    # Initialize an estimate of scene radius for Gaussian-Splatting Densification
    variables['scene_radius'] = torch.max(depth)/scene_radius_depth_ratio

    if densify_dataset is not None:
        return params, variables, intrinsics, w2c, cam, densify_intrinsics, densify_cam
    else:
        return params, variables, intrinsics, w2c, cam


def add_camera_params_to_checkpoint(params, variables, intrinsics, first_frame_w2c, 
                                    dataset_config, gt_w2c_all_frames, keyframe_time_indices):
    """
    为检查点添加相机参数，使其可以用于可视化。
    
    Args:
        params: 高斯参数字典
        variables: 包含 timestep 等变量
        intrinsics: 相机内参
        first_frame_w2c: 第一帧的相机位姿
        dataset_config: 数据集配置，包含 org_width 和 org_height
        gt_w2c_all_frames: 所有帧的GT相机位姿（到当前帧为止）
        keyframe_time_indices: 关键帧索引列表
    
    Returns:
        添加了相机参数的 params 字典
    """
    # 创建副本以避免修改原始字典
    params_with_camera = params.copy()
    
    # 添加相机参数
    params_with_camera['timestep'] = variables['timestep']
    params_with_camera['intrinsics'] = intrinsics.detach().cpu().numpy()
    params_with_camera['w2c'] = first_frame_w2c.detach().cpu().numpy()
    params_with_camera['org_width'] = dataset_config["desired_image_width"]
    params_with_camera['org_height'] = dataset_config["desired_image_height"]
    
    # 保存到当前帧为止的所有GT位姿
    params_with_camera['gt_w2c_all_frames'] = []
    for gt_w2c_tensor in gt_w2c_all_frames:
        params_with_camera['gt_w2c_all_frames'].append(gt_w2c_tensor.detach().cpu().numpy())
    params_with_camera['gt_w2c_all_frames'] = np.stack(params_with_camera['gt_w2c_all_frames'], axis=0)
    
    params_with_camera['keyframe_time_indices'] = np.array(keyframe_time_indices)
    
    return params_with_camera


def get_loss(params, curr_data, variables, iter_time_idx, loss_weights, use_sil_for_loss,
             sil_thres, use_l1, ignore_outlier_depth_loss, tracking=False, 
             mapping=False, do_ba=False, plot_dir=None, visualize_tracking_loss=False, tracking_iteration=None):
    # Initialize Loss Dictionary
    losses = {}

    if tracking:
        # Get current frame Gaussians, where only the camera pose gets gradient
        transformed_gaussians = transform_to_frame(params, iter_time_idx, 
                                             gaussians_grad=False,
                                             camera_grad=True)
    elif mapping:
        if do_ba:
            # Get current frame Gaussians, where both camera pose and Gaussians get gradient
            transformed_gaussians = transform_to_frame(params, iter_time_idx,
                                                 gaussians_grad=True,
                                                 camera_grad=True)
        else:
            # Get current frame Gaussians, where only the Gaussians get gradient
            transformed_gaussians = transform_to_frame(params, iter_time_idx,
                                                 gaussians_grad=True,
                                                 camera_grad=False)
    else:
        # Get current frame Gaussians, where only the Gaussians get gradient
        transformed_gaussians = transform_to_frame(params, iter_time_idx,
                                             gaussians_grad=True,
                                             camera_grad=False)

    # Initialize Render Variables
    rendervar = transformed_params2rendervar(params, transformed_gaussians)
    depth_sil_rendervar = transformed_params2depthplussilhouette(params, curr_data['w2c'],
                                                                 transformed_gaussians)

    # RGB Rendering
    rendervar['means2D'].retain_grad()
    im, radius, _, = Renderer(raster_settings=curr_data['cam'])(**rendervar)
    variables['means2D'] = rendervar['means2D']  # Gradient only accum from colour render for densification

    # Depth & Silhouette Rendering
    depth_sil, _, _, = Renderer(raster_settings=curr_data['cam'])(**depth_sil_rendervar)
    depth = depth_sil[0, :, :].unsqueeze(0)
    silhouette = depth_sil[1, :, :]
    presence_sil_mask = (silhouette > sil_thres)
    depth_sq = depth_sil[2, :, :].unsqueeze(0)
    uncertainty = depth_sq - depth**2
    uncertainty = uncertainty.detach()

    # Mask with valid depth values (accounts for outlier depth values)
    nan_mask = (~torch.isnan(depth)) & (~torch.isnan(uncertainty))
    if ignore_outlier_depth_loss:
        depth_error = torch.abs(curr_data['depth'] - depth) * (curr_data['depth'] > 0)
        mask = (depth_error < 10*depth_error.median())
        mask = mask & (curr_data['depth'] > 0)
    else:
        mask = (curr_data['depth'] > 0)
    mask = mask & nan_mask
    # Mask with presence silhouette mask (accounts for empty space)
    if tracking and use_sil_for_loss:
        mask = mask & presence_sil_mask

    # Depth loss
    if use_l1:
        mask = mask.detach()
        if tracking:
            losses['depth'] = torch.abs(curr_data['depth'] - depth)[mask].sum()
        else:
            losses['depth'] = torch.abs(curr_data['depth'] - depth)[mask].mean()
    
    # RGB Loss
    if tracking and (use_sil_for_loss or ignore_outlier_depth_loss):
        color_mask = torch.tile(mask, (3, 1, 1))
        color_mask = color_mask.detach()
        losses['im'] = torch.abs(curr_data['im'] - im)[color_mask].sum()
    elif tracking:
        losses['im'] = torch.abs(curr_data['im'] - im).sum()
    else:
        losses['im'] = 0.8 * l1_loss_v1(im, curr_data['im']) + 0.2 * (1.0 - calc_ssim(im, curr_data['im']))

    # [IsoGS] Added Flatness Constraint (Optimized)
    # Skip geometric constraints (Flat Loss and Iso Loss) during Tracking phase
    # These losses are only needed for Mapping to optimize Gaussian shapes
    if not tracking:
        # Get scales from log_scales
        scales = torch.exp(params['log_scales'])
        # [IsoGS] Clamp scales to prevent numerical underflow
        scales = torch.clamp(scales, min=1e-5)
        
        # [IsoGS] Debug: Print scales shape (only once)
        if not hasattr(get_loss, '_debug_printed'):
            print(f"[IsoGS DEBUG] Scales shape: {scales.shape}, Gaussian distribution type: {'Isotropic' if scales.shape[1] == 1 else 'Anisotropic'}")
            get_loss._debug_printed = True
        
        # Check dimensions and compute flatness loss
        if scales.shape[1] == 3:  # Anisotropic (3D)
            # [IsoGS] Use pure Python implementation (no torch.compile)
            loss_flat = compute_flat_loss(scales)
            losses['flat'] = loss_flat
            # [IsoGS] Compute mean max scale for monitoring
            mean_max_scale = torch.mean(torch.max(scales, dim=1).values)
            # [IsoGS] Debug print with frequency control (every 60 calls)
            if not hasattr(get_loss, '_debug_call_count'):
                get_loss._debug_call_count = 0
            get_loss._debug_call_count += 1
            if get_loss._debug_call_count % 60 == 0:
                print(f"[IsoGS Debug] Scales - Mean Min: {loss_flat.item():.6f} | Mean Max: {mean_max_scale.item():.6f} | Ratio: {mean_max_scale.item()/loss_flat.item():.1f}x")
        elif scales.shape[1] == 1:  # Isotropic (1D)
            # Skip flatness loss for isotropic Gaussians
            losses['flat'] = torch.tensor(0.0, device=scales.device, dtype=scales.dtype)
            # Print warning only once
            if not hasattr(get_loss, '_warned_isotropic'):
                print("[IsoGS Warning] Isotropic Gaussians detected. Flatness regularization skipped.")
                print("[IsoGS Solution] Please set gaussian_distribution='anisotropic' in your config file, or use the forced 3D initialization.")
                get_loss._warned_isotropic = True
        else:
            # Unexpected dimension
            losses['flat'] = torch.tensor(0.0, device=scales.device, dtype=scales.dtype)
            if not hasattr(get_loss, '_warned_unexpected_dim'):
                print(f"[IsoGS Warning] Unexpected scales dimension: {scales.shape[1]}. Flatness regularization skipped.")
                get_loss._warned_unexpected_dim = True

        # [IsoGS] Iso-Surface Density Loss (Optimized with Stochastic Sampling)
        if scales.shape[1] == 3:  # Only compute for anisotropic (3D) Gaussians
            # Parameters
            target_saturation = 1.0
            sample_size = 8192  # [IsoGS] Increased from 4096 to 8192 for better coverage while maintaining performance
            K = 16
            
            # Prepare data
            means = params['means3D']  # [N, 3]
            opacities = torch.sigmoid(params['logit_opacities'])  # [N, 1]
            
            # Build rotation matrices from quaternions
            quats = F.normalize(params['unnorm_rotations'])  # [N, 4]
            R = build_rotation(quats)  # [N, 3, 3]
            
            # Build scaling matrices (diagonal)
            # scales is already [N, 3] and clamped, represents [s_x, s_y, s_z] for each Gaussian
            # Covariance matrix: Σ = R S S^T R^T, where S = diag([s_x, s_y, s_z])
            # Since S is diagonal: S S^T = S^2 = diag([s_x^2, s_y^2, s_z^2])
            # Inverse: Σ^{-1} = R (S S^T)^{-1} R^T = R S^{-2} R^T
            
            # Compute S^{-2} = diag([1/s_x^2, 1/s_y^2, 1/s_z^2])
            S_inv_sq = 1.0 / (scales ** 2 + 1e-8)  # [N, 3], add small epsilon for numerical stability
            S_inv_sq_diag = torch.diag_embed(S_inv_sq)  # [N, 3, 3] - diagonal matrices
            
            # Compute Σ^{-1} = R S^{-2} R^T using batch matrix multiplication
            # Step 1: R @ S^{-2}: [N, 3, 3] @ [N, 3, 3] = [N, 3, 3]
            R_S_inv_sq = torch.bmm(R, S_inv_sq_diag)  # [N, 3, 3]
            # Step 2: (R @ S^{-2}) @ R^T: [N, 3, 3] @ [N, 3, 3] = [N, 3, 3]
            inverse_covariances = torch.bmm(R_S_inv_sq, R.transpose(1, 2))  # [N, 3, 3]
            
            # [IsoGS] Stochastic Sampling: Randomly sample query points each iteration
            # This ensures coverage over multiple iterations while keeping memory usage bounded
            num_gaussians = means.shape[0]
            if num_gaussians >= sample_size:
                # Random sample indices (re-sampled every iteration for better coverage)
                sample_indices = torch.randperm(num_gaussians, device=means.device)[:sample_size]
                query_points = means[sample_indices]  # [sample_size, 3]
            else:
                # If we have fewer Gaussians than sample_size, use all
                query_points = means  # [num_gaussians, 3]
                sample_size = num_gaussians
            
            # [IsoGS] Use optimized function with internal batching for density computation
            # This function handles KNN search and density calculation efficiently with chunking to avoid OOM
            loss_iso, density_val = compute_iso_surface_loss_sampled(
                query_points=query_points,
                means=means,
                inverse_covariances=inverse_covariances,
                opacities=opacities,
                K=K,
                target_saturation=target_saturation,
                chunk_size=128,  # Hard-coded internal batch size to strictly control memory usage
            )
            
            losses['iso'] = loss_iso
            
            # Store mean density for monitoring (as a scalar tensor, not part of loss computation)
            losses['mean_density'] = torch.tensor(density_val.mean().item(), device=density_val.device)
        else:
            # Skip for isotropic Gaussians
            losses['iso'] = torch.tensor(0.0, device=scales.device, dtype=scales.dtype)
    else:
        # Tracking phase: skip geometric constraints to save memory and computation
        # Only compute RGB and Depth losses for pose estimation
        losses['flat'] = torch.tensor(0.0, device=params['log_scales'].device, dtype=params['log_scales'].dtype)
        losses['iso'] = torch.tensor(0.0, device=params['log_scales'].device, dtype=params['log_scales'].dtype)

    # Visualize the Diff Images
    if tracking and visualize_tracking_loss:
        fig, ax = plt.subplots(2, 4, figsize=(12, 6))
        weighted_render_im = im * color_mask
        weighted_im = curr_data['im'] * color_mask
        weighted_render_depth = depth * mask
        weighted_depth = curr_data['depth'] * mask
        diff_rgb = torch.abs(weighted_render_im - weighted_im).mean(dim=0).detach().cpu()
        diff_depth = torch.abs(weighted_render_depth - weighted_depth).mean(dim=0).detach().cpu()
        viz_img = torch.clip(weighted_im.permute(1, 2, 0).detach().cpu(), 0, 1)
        ax[0, 0].imshow(viz_img)
        ax[0, 0].set_title("Weighted GT RGB")
        viz_render_img = torch.clip(weighted_render_im.permute(1, 2, 0).detach().cpu(), 0, 1)
        ax[1, 0].imshow(viz_render_img)
        ax[1, 0].set_title("Weighted Rendered RGB")
        ax[0, 1].imshow(weighted_depth[0].detach().cpu(), cmap="jet", vmin=0, vmax=6)
        ax[0, 1].set_title("Weighted GT Depth")
        ax[1, 1].imshow(weighted_render_depth[0].detach().cpu(), cmap="jet", vmin=0, vmax=6)
        ax[1, 1].set_title("Weighted Rendered Depth")
        ax[0, 2].imshow(diff_rgb, cmap="jet", vmin=0, vmax=0.8)
        ax[0, 2].set_title(f"Diff RGB, Loss: {torch.round(losses['im'])}")
        ax[1, 2].imshow(diff_depth, cmap="jet", vmin=0, vmax=0.8)
        ax[1, 2].set_title(f"Diff Depth, Loss: {torch.round(losses['depth'])}")
        ax[0, 3].imshow(presence_sil_mask.detach().cpu(), cmap="gray")
        ax[0, 3].set_title("Silhouette Mask")
        ax[1, 3].imshow(mask[0].detach().cpu(), cmap="gray")
        ax[1, 3].set_title("Loss Mask")
        # Turn off axis
        for i in range(2):
            for j in range(4):
                ax[i, j].axis('off')
        # Set Title
        fig.suptitle(f"Tracking Iteration: {tracking_iteration}", fontsize=16)
        # Figure Tight Layout
        fig.tight_layout()
        os.makedirs(plot_dir, exist_ok=True)
        plt.savefig(os.path.join(plot_dir, f"tmp.png"), bbox_inches='tight')
        plt.close()
        plot_img = cv2.imread(os.path.join(plot_dir, f"tmp.png"))
        cv2.imshow('Diff Images', plot_img)
        cv2.waitKey(1)
        ## Save Tracking Loss Viz
        # save_plot_dir = os.path.join(plot_dir, f"tracking_%04d" % iter_time_idx)
        # os.makedirs(save_plot_dir, exist_ok=True)
        # plt.savefig(os.path.join(save_plot_dir, f"%04d.png" % tracking_iteration), bbox_inches='tight')
        # plt.close()

    # [IsoGS] Ensure flatness loss weight exists with default value
    # Increased from 0.01 to 50.0 to match the scale of RGB Loss (~5e-3)
    # Current Flat Loss ~5e-5, so weight needs to be ~1000x to have comparable influence
    # Skip setting default weights for Tracking phase (geometric constraints not used)
    if not tracking:
        if 'flat' not in loss_weights:
            loss_weights['flat'] = 50.0
        
        # [IsoGS] Ensure iso-surface density loss weight exists with default value
        if 'iso' not in loss_weights:
            loss_weights['iso'] = 2.0
    else:
        # Tracking phase: ensure flat and iso weights are 0 (losses already set to 0)
        loss_weights['flat'] = 0.0
        loss_weights['iso'] = 0.0

    # [IsoGS] Filter out monitoring metrics (like 'mean_density') from loss computation
    # Only compute weighted losses for actual loss terms
    actual_loss_keys = ['loss', 'im', 'depth', 'flat', 'iso']
    weighted_losses = {k: v * loss_weights.get(k, 1.0) for k, v in losses.items() if k in actual_loss_keys}
    loss = sum(weighted_losses.values())

    seen = radius > 0
    variables['max_2D_radius'][seen] = torch.max(radius[seen], variables['max_2D_radius'][seen])
    variables['seen'] = seen
    weighted_losses['loss'] = loss
    
    # [IsoGS] Copy monitoring metrics (like mean_density) to weighted_losses for reporting
    if 'mean_density' in losses:
        weighted_losses['mean_density'] = losses['mean_density']

    return loss, variables, weighted_losses


def initialize_new_params(new_pt_cld, mean3_sq_dist, gaussian_distribution):
    num_pts = new_pt_cld.shape[0]
    means3D = new_pt_cld[:, :3] # [num_gaussians, 3]
    unnorm_rots = np.tile([1, 0, 0, 0], (num_pts, 1)) # [num_gaussians, 4]
    logit_opacities = torch.zeros((num_pts, 1), dtype=torch.float, device="cuda")
    # [IsoGS] Force 3D (anisotropic) initialization for flatness regularization
    # Even if config says "isotropic", we initialize as 3D with small random perturbation
    base_log_scale = torch.log(torch.sqrt(mean3_sq_dist))[..., None]  # [num_pts, 1]
    if gaussian_distribution == "isotropic":
        # Convert to 3D by tiling and adding small random perturbation to break symmetry
        # This allows flatness regularization to work even with "isotropic" config
        log_scales_base = torch.tile(base_log_scale, (1, 3))  # [num_pts, 3]
        # Add small random perturbation (std=0.01) to prevent identical gradients
        perturbation = torch.randn_like(log_scales_base) * 0.01
        log_scales = log_scales_base + perturbation
    elif gaussian_distribution == "anisotropic":
        log_scales = torch.tile(base_log_scale, (1, 3))  # [num_pts, 3]
    else:
        raise ValueError(f"Unknown gaussian_distribution {gaussian_distribution}")
    params = {
        'means3D': means3D,
        'rgb_colors': new_pt_cld[:, 3:6],
        'unnorm_rotations': unnorm_rots,
        'logit_opacities': logit_opacities,
        'log_scales': log_scales,
    }
    for k, v in params.items():
        # Check if value is already a torch tensor
        if not isinstance(v, torch.Tensor):
            params[k] = torch.nn.Parameter(torch.tensor(v).cuda().float().contiguous().requires_grad_(True))
        else:
            params[k] = torch.nn.Parameter(v.cuda().float().contiguous().requires_grad_(True))

    return params


def add_new_gaussians(params, variables, curr_data, sil_thres, 
                      time_idx, mean_sq_dist_method, gaussian_distribution):
    # Silhouette Rendering
    transformed_gaussians = transform_to_frame(params, time_idx, gaussians_grad=False, camera_grad=False)
    depth_sil_rendervar = transformed_params2depthplussilhouette(params, curr_data['w2c'],
                                                                 transformed_gaussians)
    depth_sil, _, _, = Renderer(raster_settings=curr_data['cam'])(**depth_sil_rendervar)
    silhouette = depth_sil[1, :, :]
    non_presence_sil_mask = (silhouette < sil_thres)
    # Check for new foreground objects by using GT depth
    gt_depth = curr_data['depth'][0, :, :]
    render_depth = depth_sil[0, :, :]
    depth_error = torch.abs(gt_depth - render_depth) * (gt_depth > 0)
    non_presence_depth_mask = (render_depth > gt_depth) * (depth_error > 50*depth_error.median())
    # Determine non-presence mask
    non_presence_mask = non_presence_sil_mask | non_presence_depth_mask
    # Flatten mask
    non_presence_mask = non_presence_mask.reshape(-1)

    # Get the new frame Gaussians based on the Silhouette
    if torch.sum(non_presence_mask) > 0:
        # Get the new pointcloud in the world frame
        curr_cam_rot = torch.nn.functional.normalize(params['cam_unnorm_rots'][..., time_idx].detach())
        curr_cam_tran = params['cam_trans'][..., time_idx].detach()
        curr_w2c = torch.eye(4).cuda().float()
        curr_w2c[:3, :3] = build_rotation(curr_cam_rot)
        curr_w2c[:3, 3] = curr_cam_tran
        valid_depth_mask = (curr_data['depth'][0, :, :] > 0)
        non_presence_mask = non_presence_mask & valid_depth_mask.reshape(-1)
        new_pt_cld, mean3_sq_dist = get_pointcloud(curr_data['im'], curr_data['depth'], curr_data['intrinsics'], 
                                    curr_w2c, mask=non_presence_mask, compute_mean_sq_dist=True,
                                    mean_sq_dist_method=mean_sq_dist_method)
        new_params = initialize_new_params(new_pt_cld, mean3_sq_dist, gaussian_distribution)
        for k, v in new_params.items():
            params[k] = torch.nn.Parameter(torch.cat((params[k], v), dim=0).requires_grad_(True))
        num_pts = params['means3D'].shape[0]
        variables['means2D_gradient_accum'] = torch.zeros(num_pts, device="cuda").float()
        variables['denom'] = torch.zeros(num_pts, device="cuda").float()
        variables['max_2D_radius'] = torch.zeros(num_pts, device="cuda").float()
        new_timestep = time_idx*torch.ones(new_pt_cld.shape[0],device="cuda").float()
        variables['timestep'] = torch.cat((variables['timestep'],new_timestep),dim=0)

    return params, variables


def initialize_camera_pose(params, curr_time_idx, forward_prop):
    with torch.no_grad():
        if curr_time_idx > 1 and forward_prop:
            # Initialize the camera pose for the current frame based on a constant velocity model
            # Rotation
            prev_rot1 = F.normalize(params['cam_unnorm_rots'][..., curr_time_idx-1].detach())
            prev_rot2 = F.normalize(params['cam_unnorm_rots'][..., curr_time_idx-2].detach())
            new_rot = F.normalize(prev_rot1 + (prev_rot1 - prev_rot2))
            params['cam_unnorm_rots'][..., curr_time_idx] = new_rot.detach()
            # Translation
            prev_tran1 = params['cam_trans'][..., curr_time_idx-1].detach()
            prev_tran2 = params['cam_trans'][..., curr_time_idx-2].detach()
            new_tran = prev_tran1 + (prev_tran1 - prev_tran2)
            params['cam_trans'][..., curr_time_idx] = new_tran.detach()
        else:
            # Initialize the camera pose for the current frame
            params['cam_unnorm_rots'][..., curr_time_idx] = params['cam_unnorm_rots'][..., curr_time_idx-1].detach()
            params['cam_trans'][..., curr_time_idx] = params['cam_trans'][..., curr_time_idx-1].detach()
    
    return params


def convert_params_to_store(params):
    params_to_store = {}
    for k, v in params.items():
        if isinstance(v, torch.Tensor):
            params_to_store[k] = v.detach().clone()
        else:
            params_to_store[k] = v
    return params_to_store


def rgbd_slam(config: dict):
    # Print Config
    print("Loaded Config:")
    if "use_depth_loss_thres" not in config['tracking']:
        config['tracking']['use_depth_loss_thres'] = False
        config['tracking']['depth_loss_thres'] = 100000
    if "visualize_tracking_loss" not in config['tracking']:
        config['tracking']['visualize_tracking_loss'] = False
    if "gaussian_distribution" not in config:
        config['gaussian_distribution'] = "isotropic"
    print(f"{config}")

    # Create Output Directories
    output_dir = os.path.join(config["workdir"], config["run_name"])
    eval_dir = os.path.join(output_dir, "eval")
    os.makedirs(eval_dir, exist_ok=True)
    # [IsoGS] 初始化指标 CSV 日志路径（具体截断逻辑在确定 checkpoint_time_idx 后再执行）
    metrics_csv_path = None
    metrics_fieldnames = None
    
    # Init WandB
    if config['use_wandb']:
        wandb_time_step = 0
        wandb_tracking_step = 0
        wandb_mapping_step = 0
        wandb_run = wandb.init(project=config['wandb']['project'],
                               entity=config['wandb']['entity'],
                               group=config['wandb']['group'],
                               name=config['wandb']['name'],
                               config=config)
    else:
        # [IsoGS] Initialize step counters even when wandb is disabled
        wandb_time_step = 0
        wandb_tracking_step = 0
        wandb_mapping_step = 0
        wandb_run = None

    # Get Device
    device = torch.device(config["primary_device"])

    # Load Dataset
    print("Loading Dataset ...")
    dataset_config = config["data"]
    if "gradslam_data_cfg" not in dataset_config:
        gradslam_data_cfg = {}
        gradslam_data_cfg["dataset_name"] = dataset_config["dataset_name"]
    else:
        gradslam_data_cfg = load_dataset_config(dataset_config["gradslam_data_cfg"])
    if "ignore_bad" not in dataset_config:
        dataset_config["ignore_bad"] = False
    if "use_train_split" not in dataset_config:
        dataset_config["use_train_split"] = True
    if "densification_image_height" not in dataset_config:
        dataset_config["densification_image_height"] = dataset_config["desired_image_height"]
        dataset_config["densification_image_width"] = dataset_config["desired_image_width"]
        seperate_densification_res = False
    else:
        if dataset_config["densification_image_height"] != dataset_config["desired_image_height"] or \
            dataset_config["densification_image_width"] != dataset_config["desired_image_width"]:
            seperate_densification_res = True
        else:
            seperate_densification_res = False
    if "tracking_image_height" not in dataset_config:
        dataset_config["tracking_image_height"] = dataset_config["desired_image_height"]
        dataset_config["tracking_image_width"] = dataset_config["desired_image_width"]
        seperate_tracking_res = False
    else:
        if dataset_config["tracking_image_height"] != dataset_config["desired_image_height"] or \
            dataset_config["tracking_image_width"] != dataset_config["desired_image_width"]:
            seperate_tracking_res = True
        else:
            seperate_tracking_res = False
    # Poses are relative to the first frame
    dataset = get_dataset(
        config_dict=gradslam_data_cfg,
        basedir=dataset_config["basedir"],
        sequence=os.path.basename(dataset_config["sequence"]),
        start=dataset_config["start"],
        end=dataset_config["end"],
        stride=dataset_config["stride"],
        desired_height=dataset_config["desired_image_height"],
        desired_width=dataset_config["desired_image_width"],
        device=device,
        relative_pose=True,
        ignore_bad=dataset_config["ignore_bad"],
        use_train_split=dataset_config["use_train_split"],
    )
    num_frames = dataset_config["num_frames"]
    if num_frames == -1:
        num_frames = len(dataset)

    # [IsoGS] 可选的结束帧（由命令行 --end-at 传入，挂在 config['end_at'] 上）
    end_at = config.get("end_at", None)

    # Init seperate dataloader for densification if required
    if seperate_densification_res:
        densify_dataset = get_dataset(
            config_dict=gradslam_data_cfg,
            basedir=dataset_config["basedir"],
            sequence=os.path.basename(dataset_config["sequence"]),
            start=dataset_config["start"],
            end=dataset_config["end"],
            stride=dataset_config["stride"],
            desired_height=dataset_config["densification_image_height"],
            desired_width=dataset_config["densification_image_width"],
            device=device,
            relative_pose=True,
            ignore_bad=dataset_config["ignore_bad"],
            use_train_split=dataset_config["use_train_split"],
        )
        # Initialize Parameters, Canonical & Densification Camera parameters
        params, variables, intrinsics, first_frame_w2c, cam, \
            densify_intrinsics, densify_cam = initialize_first_timestep(dataset, num_frames,
                                                                        config['scene_radius_depth_ratio'],
                                                                        config['mean_sq_dist_method'],
                                                                        densify_dataset=densify_dataset,
                                                                        gaussian_distribution=config['gaussian_distribution'])                                                                                                                  
    else:
        # Initialize Parameters & Canoncial Camera parameters
        params, variables, intrinsics, first_frame_w2c, cam = initialize_first_timestep(dataset, num_frames, 
                                                                                        config['scene_radius_depth_ratio'],
                                                                                        config['mean_sq_dist_method'],
                                                                                        gaussian_distribution=config['gaussian_distribution'])
    
    # Init seperate dataloader for tracking if required
    if seperate_tracking_res:
        tracking_dataset = get_dataset(
            config_dict=gradslam_data_cfg,
            basedir=dataset_config["basedir"],
            sequence=os.path.basename(dataset_config["sequence"]),
            start=dataset_config["start"],
            end=dataset_config["end"],
            stride=dataset_config["stride"],
            desired_height=dataset_config["tracking_image_height"],
            desired_width=dataset_config["tracking_image_width"],
            device=device,
            relative_pose=True,
            ignore_bad=dataset_config["ignore_bad"],
            use_train_split=dataset_config["use_train_split"],
        )
        tracking_color, _, tracking_intrinsics, _ = tracking_dataset[0]
        tracking_color = tracking_color.permute(2, 0, 1) / 255 # (H, W, C) -> (C, H, W)
        tracking_intrinsics = tracking_intrinsics[:3, :3]
        tracking_cam = setup_camera(tracking_color.shape[2], tracking_color.shape[1], 
                                    tracking_intrinsics.cpu().numpy(), first_frame_w2c.detach().cpu().numpy())
    
    # Initialize list to keep track of Keyframes
    keyframe_list = []
    keyframe_time_indices = []
    
    # Init Variables to keep track of ground truth poses and runtimes
    gt_w2c_all_frames = []
    tracking_iter_time_sum = 0
    tracking_iter_time_count = 0
    mapping_iter_time_sum = 0
    mapping_iter_time_count = 0
    tracking_frame_time_sum = 0
    tracking_frame_time_count = 0
    mapping_frame_time_sum = 0
    mapping_frame_time_count = 0

    # Load Checkpoint
    checkpoint_time_idx = 0
    if config['load_checkpoint']:
        checkpoint_time_idx = config.get('checkpoint_time_idx', 0)

        # checkpoint_time_idx < 0 表示自动从当前 run 目录中最新的 params*.npz 继续
        if checkpoint_time_idx < 0:
            ckpt_output_dir = os.path.join(config["workdir"], config["run_name"])
            latest_idx = None
            try:
                if os.path.isdir(ckpt_output_dir):
                    for fname in os.listdir(ckpt_output_dir):
                        if fname.startswith("params") and fname.endswith(".npz"):
                            num_str = fname[len("params"):-len(".npz")]
                            if num_str.isdigit():
                                idx = int(num_str)
                                if (latest_idx is None) or (idx > latest_idx):
                                    latest_idx = idx
            except Exception as e:
                print(f"[Checkpoint Auto-Select Warning] Failed to scan directory {ckpt_output_dir}: {e}")

            if latest_idx is not None:
                checkpoint_time_idx = latest_idx
                print(f"[Checkpoint] Auto-selected latest checkpoint frame: {checkpoint_time_idx}")
            else:
                # 没有找到任何 checkpoint，直接从头开始
                print("[Checkpoint] No existing checkpoints found, starting from frame 0.")
                config['load_checkpoint'] = False
                checkpoint_time_idx = 0

        if config['load_checkpoint']:
            print(f"Loading Checkpoint for Frame {checkpoint_time_idx}")
            ckpt_path = os.path.join(config['workdir'], config['run_name'], f"params{checkpoint_time_idx}.npz")
            params = dict(np.load(ckpt_path, allow_pickle=True))
            
            # 如果检查点中有 timestep，先保存到 variables，然后从 params 中移除
            # timestep 是标量值，不是需要优化的参数
            if 'timestep' in params:
                variables['timestep'] = torch.tensor(params['timestep']).cuda().float()
                params.pop('timestep')
            
            # 移除其他相机参数，这些不应该被优化器处理
            # 这些参数在保存检查点时被添加，但加载后需要移除
            camera_params_to_remove = ['intrinsics', 'w2c', 'org_width', 'org_height', 
                                      'gt_w2c_all_frames', 'keyframe_time_indices']
            for k in camera_params_to_remove:
                if k in params:
                    params.pop(k)
            
            # 转换为 torch tensor 并设置 requires_grad
            params = {k: torch.tensor(params[k]).cuda().float().requires_grad_(True) for k in params.keys()}
            
            # 获取当前高斯点的数量
            current_num_gaussians = params['means3D'].shape[0]
            
            # 初始化 variables
            variables['max_2D_radius'] = torch.zeros(current_num_gaussians).cuda().float()
            variables['means2D_gradient_accum'] = torch.zeros(current_num_gaussians).cuda().float()
            variables['denom'] = torch.zeros(current_num_gaussians).cuda().float()
            
            # 处理 timestep：确保它的形状与当前高斯点数量匹配
            # timestep 记录每个高斯点是在哪一帧创建的，必须与高斯点数量一一对应
            if 'timestep' in variables:
                # 如果从检查点加载了 timestep，检查形状是否匹配
                if variables['timestep'].shape[0] != current_num_gaussians:
                    old_size = variables['timestep'].shape[0]
                    # 形状不匹配：这不应该发生，因为检查点保存时应该包含所有高斯点
                    # 可能的原因：
                    # 1. 检查点文件损坏/不完整
                    # 2. 检查点保存后，在保存和加载之间添加了新点（不应该发生）
                    # 3. 检查点保存逻辑有问题
                    print(f"[Warning] timestep size mismatch detected!")
                    print(f"  Checkpoint timestep size: {old_size}")
                    print(f"  Current params size: {current_num_gaussians}")
                    print(f"  Checkpoint frame: {checkpoint_time_idx}")
                    
                    if old_size < current_num_gaussians:
                        # 检查点中的 timestep 小于当前高斯点数量
                        # 最安全的做法：重新初始化所有 timestep，使用检查点帧号作为默认值
                        # 这样虽然丢失了原始时间步信息，但至少保证了一致性
                        # 新添加的点会通过 add_new_gaussians 获得正确的时间步
                        print(f"[Warning] Reinitializing timestep to match current size")
                        print(f"[Warning] All points will be marked as created at frame {checkpoint_time_idx}")
                        print(f"[Warning] New points added later will get correct timestep via add_new_gaussians")
                        variables['timestep'] = torch.full(
                            (current_num_gaussians,), 
                            float(checkpoint_time_idx), 
                            device="cuda"
                        ).float()
                    else:
                        # 如果旧的大小更大（不应该发生），截断它
                        print(f"[Warning] Truncating timestep from {old_size} to {current_num_gaussians}")
                        variables['timestep'] = variables['timestep'][:current_num_gaussians]
            else:
                # 如果没有从检查点加载 timestep，初始化为检查点帧号
                # 这样所有点都被标记为在检查点帧创建
                variables['timestep'] = torch.full(
                    (current_num_gaussians,), 
                    float(checkpoint_time_idx), 
                    device="cuda"
                ).float()
            # Load the keyframe time idx list
            keyframe_path = os.path.join(
                config['workdir'],
                config['run_name'],
                f"keyframe_time_indices{checkpoint_time_idx}.npy",
            )
            if os.path.exists(keyframe_path):
                keyframe_time_indices = np.load(keyframe_path)
                keyframe_time_indices = keyframe_time_indices.tolist()
            else:
                print(f"[Checkpoint Warning] Keyframe index file not found: {keyframe_path}")
                keyframe_time_indices = []

            # Update the ground truth poses list
            for time_idx in range(checkpoint_time_idx):
                # Load RGBD frames incrementally instead of all frames
                color, depth, _, gt_pose = dataset[time_idx]
                # Process poses
                gt_w2c = torch.linalg.inv(gt_pose)
                gt_w2c_all_frames.append(gt_w2c)
                # Initialize Keyframe List
                if time_idx in keyframe_time_indices:
                    # Get the estimated rotation & translation
                    curr_cam_rot = F.normalize(params['cam_unnorm_rots'][..., time_idx].detach())
                    curr_cam_tran = params['cam_trans'][..., time_idx].detach()
                    curr_w2c = torch.eye(4).cuda().float()
                    curr_w2c[:3, :3] = build_rotation(curr_cam_rot)
                    curr_w2c[:3, 3] = curr_cam_tran
                    # Initialize Keyframe Info
                    color = color.permute(2, 0, 1) / 255
                    depth = depth.permute(2, 0, 1)
                    curr_keyframe = {'id': time_idx, 'est_w2c': curr_w2c, 'color': color, 'depth': depth}
                    # Add to keyframe list
                    keyframe_list.append(curr_keyframe)

    # [IsoGS] 在确定最终的 checkpoint_time_idx 之后，初始化/截断指标 CSV
    metrics_csv_path, metrics_fieldnames = _init_metrics_csv(output_dir, checkpoint_time_idx)

    # [IsoGS] 计算这次运行的实际结束帧（end_frame），并处理 “已跑完” 的情况
    end_frame = num_frames - 1  # 默认最后一帧索引（0-based）
    if end_at is not None:
        # end_at 是帧号（0-based 或 1-based）的终止上限：在 case2 中你已经允许“对齐到最近的 checkpoint”
        # 这里只做简单裁剪，实际保存仍由 checkpoint_interval 控制
        if end_at < checkpoint_time_idx:
            # Case4: 用户要求的 end_at 小于已经存在的 checkpoint 帧
            print(f"[End-At] You have already finished at frame {checkpoint_time_idx}. Requested end_at={end_at}. Nothing to do.")
            return
        # 将 end_frame 裁剪到 [checkpoint_time_idx, num_frames-1] 区间内
        end_frame = min(int(end_at), num_frames - 1)

    if checkpoint_time_idx >= num_frames:
        # 数据本身已经全跑完
        print(f"[End-At] Dataset already fully processed. checkpoint_time_idx={checkpoint_time_idx}, num_frames={num_frames}")
        return

    if end_frame <= checkpoint_time_idx:
        print(f"[End-At] Nothing to do. checkpoint_time_idx={checkpoint_time_idx}, end_frame={end_frame}")
        return

    print(f"[End-At] Will process frames from {checkpoint_time_idx} to {end_frame} (inclusive).")

    # Iterate over Scan
    for time_idx in tqdm(range(checkpoint_time_idx, end_frame + 1)):
        # Load RGBD frames incrementally instead of all frames
        color, depth, _, gt_pose = dataset[time_idx]
        # Process poses
        gt_w2c = torch.linalg.inv(gt_pose)
        # Process RGB-D Data
        color = color.permute(2, 0, 1) / 255
        depth = depth.permute(2, 0, 1)
        gt_w2c_all_frames.append(gt_w2c)
        curr_gt_w2c = gt_w2c_all_frames
        # Optimize only current time step for tracking
        iter_time_idx = time_idx
        # Initialize Mapping Data for selected frame
        curr_data = {'cam': cam, 'im': color, 'depth': depth, 'id': iter_time_idx, 'intrinsics': intrinsics, 
                     'w2c': first_frame_w2c, 'iter_gt_w2c_list': curr_gt_w2c}
        
        # Initialize Data for Tracking
        if seperate_tracking_res:
            tracking_color, tracking_depth, _, _ = tracking_dataset[time_idx]
            tracking_color = tracking_color.permute(2, 0, 1) / 255
            tracking_depth = tracking_depth.permute(2, 0, 1)
            tracking_curr_data = {'cam': tracking_cam, 'im': tracking_color, 'depth': tracking_depth, 'id': iter_time_idx,
                                  'intrinsics': tracking_intrinsics, 'w2c': first_frame_w2c, 'iter_gt_w2c_list': curr_gt_w2c}
        else:
            tracking_curr_data = curr_data

        # Optimization Iterations
        num_iters_mapping = config['mapping']['num_iters']
        
        # Initialize the camera pose for the current frame
        if time_idx > 0:
            params = initialize_camera_pose(params, time_idx, forward_prop=config['tracking']['forward_prop'])

        # Tracking
        tracking_start_time = time.time()
        if time_idx > 0 and not config['tracking']['use_gt_poses']:
            # Reset Optimizer & Learning Rates for tracking
            optimizer = initialize_optimizer(params, config['tracking']['lrs'], tracking=True)
            # Keep Track of Best Candidate Rotation & Translation
            candidate_cam_unnorm_rot = params['cam_unnorm_rots'][..., time_idx].detach().clone()
            candidate_cam_tran = params['cam_trans'][..., time_idx].detach().clone()
            current_min_loss = float(1e20)
            # Tracking Optimization
            iter = 0
            do_continue_slam = False
            num_iters_tracking = config['tracking']['num_iters']
            progress_bar = tqdm(range(num_iters_tracking), desc=f"Tracking Time Step: {time_idx}")
            while True:
                iter_start_time = time.time()
                # Loss for current frame
                loss, variables, losses = get_loss(
                    params,
                    tracking_curr_data,
                    variables,
                    iter_time_idx,
                    config['tracking']['loss_weights'],
                    config['tracking']['use_sil_for_loss'],
                    config['tracking']['sil_thres'],
                    config['tracking']['use_l1'],
                    config['tracking']['ignore_outlier_depth_loss'],
                    tracking=True,
                    plot_dir=eval_dir,
                    visualize_tracking_loss=config['tracking']['visualize_tracking_loss'],
                    tracking_iteration=iter,
                )
                if config['use_wandb']:
                    # Report Loss
                    wandb_tracking_step = report_loss(losses, wandb_run, wandb_tracking_step, tracking=True)
                else:
                    # 即便不使用 wandb，也通过 report_loss 打印并驱动 CSV 记录
                    wandb_tracking_step = report_loss(losses, None, wandb_tracking_step, tracking=True)
                # [IsoGS] 记录 Tracking 指标到 CSV（frame = 当前 time_idx）
                _append_metrics_row(
                    metrics_csv_path,
                    metrics_fieldnames,
                    frame_idx=time_idx,
                    stage="tracking",
                    step_idx=max(wandb_tracking_step - 1, 0),
                    losses=losses,
                )
                # Backprop
                loss.backward()
                # Optimizer Update
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                with torch.no_grad():
                    # Save the best candidate rotation & translation
                    if loss < current_min_loss:
                        current_min_loss = loss
                        candidate_cam_unnorm_rot = params['cam_unnorm_rots'][..., time_idx].detach().clone()
                        candidate_cam_tran = params['cam_trans'][..., time_idx].detach().clone()
                    # Report Progress
                    if config['report_iter_progress']:
                        if config['use_wandb']:
                            report_progress(params, tracking_curr_data, iter+1, progress_bar, iter_time_idx, sil_thres=config['tracking']['sil_thres'], tracking=True,
                                            wandb_run=wandb_run, wandb_step=wandb_tracking_step, wandb_save_qual=config['wandb']['save_qual'])
                        else:
                            report_progress(params, tracking_curr_data, iter+1, progress_bar, iter_time_idx, sil_thres=config['tracking']['sil_thres'], tracking=True)
                    else:
                        progress_bar.update(1)
                # Update the runtime numbers
                iter_end_time = time.time()
                tracking_iter_time_sum += iter_end_time - iter_start_time
                tracking_iter_time_count += 1
                # Check if we should stop tracking
                iter += 1
                if iter == num_iters_tracking:
                    if losses['depth'] < config['tracking']['depth_loss_thres'] and config['tracking']['use_depth_loss_thres']:
                        break
                    elif config['tracking']['use_depth_loss_thres'] and not do_continue_slam:
                        do_continue_slam = True
                        progress_bar = tqdm(range(num_iters_tracking), desc=f"Tracking Time Step: {time_idx}")
                        num_iters_tracking = 2*num_iters_tracking
                        if config['use_wandb']:
                            wandb_run.log({"Tracking/Extra Tracking Iters Frames": time_idx,
                                        "Tracking/step": wandb_time_step})
                    else:
                        break

            progress_bar.close()
            # Copy over the best candidate rotation & translation
            with torch.no_grad():
                params['cam_unnorm_rots'][..., time_idx] = candidate_cam_unnorm_rot
                params['cam_trans'][..., time_idx] = candidate_cam_tran
        elif time_idx > 0 and config['tracking']['use_gt_poses']:
            with torch.no_grad():
                # Get the ground truth pose relative to frame 0
                rel_w2c = curr_gt_w2c[-1]
                rel_w2c_rot = rel_w2c[:3, :3].unsqueeze(0).detach()
                rel_w2c_rot_quat = matrix_to_quaternion(rel_w2c_rot)
                rel_w2c_tran = rel_w2c[:3, 3].detach()
                # Update the camera parameters
                params['cam_unnorm_rots'][..., time_idx] = rel_w2c_rot_quat
                params['cam_trans'][..., time_idx] = rel_w2c_tran
        # Update the runtime numbers
        tracking_end_time = time.time()
        tracking_frame_time_sum += tracking_end_time - tracking_start_time
        tracking_frame_time_count += 1

        if time_idx == 0 or (time_idx+1) % config['report_global_progress_every'] == 0:
            try:
                # Report Final Tracking Progress
                progress_bar = tqdm(range(1), desc=f"Tracking Result Time Step: {time_idx}")
                with torch.no_grad():
                    if config['use_wandb']:
                        report_progress(params, tracking_curr_data, 1, progress_bar, iter_time_idx, sil_thres=config['tracking']['sil_thres'], tracking=True,
                                        wandb_run=wandb_run, wandb_step=wandb_time_step, wandb_save_qual=config['wandb']['save_qual'], global_logging=True)
                    else:
                        report_progress(params, tracking_curr_data, 1, progress_bar, iter_time_idx, sil_thres=config['tracking']['sil_thres'], tracking=True)
                progress_bar.close()
            except:
                ckpt_output_dir = os.path.join(config["workdir"], config["run_name"])
                # 添加相机参数以便检查点可以用于可视化
                params_with_camera = add_camera_params_to_checkpoint(
                    params, variables, intrinsics, first_frame_w2c, 
                    dataset_config, gt_w2c_all_frames, keyframe_time_indices
                )
                save_params_ckpt(params_with_camera, ckpt_output_dir, time_idx)
                print('Failed to evaluate trajectory.')

        # Densification & KeyFrame-based Mapping
        if time_idx == 0 or (time_idx+1) % config['map_every'] == 0:
            # Densification
            if config['mapping']['add_new_gaussians'] and time_idx > 0:
                # Setup Data for Densification
                if seperate_densification_res:
                    # Load RGBD frames incrementally instead of all frames
                    densify_color, densify_depth, _, _ = densify_dataset[time_idx]
                    densify_color = densify_color.permute(2, 0, 1) / 255
                    densify_depth = densify_depth.permute(2, 0, 1)
                    densify_curr_data = {'cam': densify_cam, 'im': densify_color, 'depth': densify_depth, 'id': time_idx, 
                                 'intrinsics': densify_intrinsics, 'w2c': first_frame_w2c, 'iter_gt_w2c_list': curr_gt_w2c}
                else:
                    densify_curr_data = curr_data

                # Add new Gaussians to the scene based on the Silhouette
                params, variables = add_new_gaussians(params, variables, densify_curr_data, 
                                                      config['mapping']['sil_thres'], time_idx,
                                                      config['mean_sq_dist_method'], config['gaussian_distribution'])
                post_num_pts = params['means3D'].shape[0]
                if config['use_wandb']:
                    wandb_run.log({"Mapping/Number of Gaussians": post_num_pts,
                                   "Mapping/step": wandb_time_step})
            
            with torch.no_grad():
                # Get the current estimated rotation & translation
                curr_cam_rot = F.normalize(params['cam_unnorm_rots'][..., time_idx].detach())
                curr_cam_tran = params['cam_trans'][..., time_idx].detach()
                curr_w2c = torch.eye(4).cuda().float()
                curr_w2c[:3, :3] = build_rotation(curr_cam_rot)
                curr_w2c[:3, 3] = curr_cam_tran
                # Select Keyframes for Mapping
                num_keyframes = config['mapping_window_size']-2
                selected_keyframes = keyframe_selection_overlap(depth, curr_w2c, intrinsics, keyframe_list[:-1], num_keyframes)
                selected_time_idx = [keyframe_list[frame_idx]['id'] for frame_idx in selected_keyframes]
                if len(keyframe_list) > 0:
                    # Add last keyframe to the selected keyframes
                    selected_time_idx.append(keyframe_list[-1]['id'])
                    selected_keyframes.append(len(keyframe_list)-1)
                # Add current frame to the selected keyframes
                selected_time_idx.append(time_idx)
                selected_keyframes.append(-1)
                # Print the selected keyframes
                print(f"\nSelected Keyframes at Frame {time_idx}: {selected_time_idx}")

            # Reset Optimizer & Learning Rates for Full Map Optimization
            optimizer = initialize_optimizer(params, config['mapping']['lrs'], tracking=False) 

            # Mapping
            mapping_start_time = time.time()
            if num_iters_mapping > 0:
                progress_bar = tqdm(range(num_iters_mapping), desc=f"Mapping Time Step: {time_idx}")
            for iter in range(num_iters_mapping):
                iter_start_time = time.time()
                # Randomly select a frame until current time step amongst keyframes
                rand_idx = np.random.randint(0, len(selected_keyframes))
                selected_rand_keyframe_idx = selected_keyframes[rand_idx]
                if selected_rand_keyframe_idx == -1:
                    # Use Current Frame Data
                    iter_time_idx = time_idx
                    iter_color = color
                    iter_depth = depth
                else:
                    # Use Keyframe Data
                    iter_time_idx = keyframe_list[selected_rand_keyframe_idx]['id']
                    iter_color = keyframe_list[selected_rand_keyframe_idx]['color']
                    iter_depth = keyframe_list[selected_rand_keyframe_idx]['depth']
                iter_gt_w2c = gt_w2c_all_frames[:iter_time_idx+1]
                iter_data = {'cam': cam, 'im': iter_color, 'depth': iter_depth, 'id': iter_time_idx, 
                             'intrinsics': intrinsics, 'w2c': first_frame_w2c, 'iter_gt_w2c_list': iter_gt_w2c}
                # Loss for current frame
                loss, variables, losses = get_loss(
                    params,
                    iter_data,
                    variables,
                    iter_time_idx,
                    config['mapping']['loss_weights'],
                    config['mapping']['use_sil_for_loss'],
                    config['mapping']['sil_thres'],
                    config['mapping']['use_l1'],
                    config['mapping']['ignore_outlier_depth_loss'],
                    mapping=True,
                )
                # [IsoGS] Report loss regardless of wandb status
                if config['use_wandb']:
                    # Report Loss
                    wandb_mapping_step = report_loss(losses, wandb_run, wandb_mapping_step, mapping=True)
                else:
                    # Call report_loss with None wandb_run to enable terminal printing
                    wandb_mapping_step = report_loss(losses, None, wandb_mapping_step, mapping=True)
                # [IsoGS] 记录 Mapping 指标到 CSV（frame = 当前 time_idx）
                _append_metrics_row(
                    metrics_csv_path,
                    metrics_fieldnames,
                    frame_idx=time_idx,
                    stage="mapping",
                    step_idx=max(wandb_mapping_step - 1, 0),
                    losses=losses,
                )
                # Backprop
                loss.backward()
                with torch.no_grad():
                    # Prune Gaussians
                    if config['mapping']['prune_gaussians']:
                        params, variables = prune_gaussians(params, variables, optimizer, iter, config['mapping']['pruning_dict'])
                        if config['use_wandb']:
                            wandb_run.log({"Mapping/Number of Gaussians - Pruning": params['means3D'].shape[0],
                                           "Mapping/step": wandb_mapping_step})
                    # Gaussian-Splatting's Gradient-based Densification
                    if config['mapping']['use_gaussian_splatting_densification']:
                        params, variables = densify(params, variables, optimizer, iter, config['mapping']['densify_dict'])
                        if config['use_wandb']:
                            wandb_run.log({"Mapping/Number of Gaussians - Densification": params['means3D'].shape[0],
                                           "Mapping/step": wandb_mapping_step})
                    # Optimizer Update
                    optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                    # Report Progress
                    if config['report_iter_progress']:
                        if config['use_wandb']:
                            report_progress(params, iter_data, iter+1, progress_bar, iter_time_idx, sil_thres=config['mapping']['sil_thres'], 
                                            wandb_run=wandb_run, wandb_step=wandb_mapping_step, wandb_save_qual=config['wandb']['save_qual'],
                                            mapping=True, online_time_idx=time_idx)
                        else:
                            report_progress(params, iter_data, iter+1, progress_bar, iter_time_idx, sil_thres=config['mapping']['sil_thres'], 
                                            mapping=True, online_time_idx=time_idx)
                    else:
                        progress_bar.update(1)
                # Update the runtime numbers
                iter_end_time = time.time()
                mapping_iter_time_sum += iter_end_time - iter_start_time
                mapping_iter_time_count += 1
            if num_iters_mapping > 0:
                progress_bar.close()
            # Update the runtime numbers
            mapping_end_time = time.time()
            mapping_frame_time_sum += mapping_end_time - mapping_start_time
            mapping_frame_time_count += 1

            if time_idx == 0 or (time_idx+1) % config['report_global_progress_every'] == 0:
                try:
                    # Report Mapping Progress
                    progress_bar = tqdm(range(1), desc=f"Mapping Result Time Step: {time_idx}")
                    with torch.no_grad():
                        if config['use_wandb']:
                            report_progress(params, curr_data, 1, progress_bar, time_idx, sil_thres=config['mapping']['sil_thres'], 
                                            wandb_run=wandb_run, wandb_step=wandb_time_step, wandb_save_qual=config['wandb']['save_qual'],
                                            mapping=True, online_time_idx=time_idx, global_logging=True)
                        else:
                            report_progress(params, curr_data, 1, progress_bar, time_idx, sil_thres=config['mapping']['sil_thres'], 
                                            mapping=True, online_time_idx=time_idx)
                    progress_bar.close()
                except:
                    ckpt_output_dir = os.path.join(config["workdir"], config["run_name"])
                    # 添加相机参数以便检查点可以用于可视化
                    params_with_camera = add_camera_params_to_checkpoint(
                        params, variables, intrinsics, first_frame_w2c, 
                        dataset_config, gt_w2c_all_frames, keyframe_time_indices
                    )
                    save_params_ckpt(params_with_camera, ckpt_output_dir, time_idx)
                    print('Failed to evaluate trajectory.')
        
        # Add frame to keyframe list
        if ((time_idx == 0) or ((time_idx+1) % config['keyframe_every'] == 0) or \
                    (time_idx == num_frames-2)) and (not torch.isinf(curr_gt_w2c[-1]).any()) and (not torch.isnan(curr_gt_w2c[-1]).any()):
            with torch.no_grad():
                # Get the current estimated rotation & translation
                curr_cam_rot = F.normalize(params['cam_unnorm_rots'][..., time_idx].detach())
                curr_cam_tran = params['cam_trans'][..., time_idx].detach()
                curr_w2c = torch.eye(4).cuda().float()
                curr_w2c[:3, :3] = build_rotation(curr_cam_rot)
                curr_w2c[:3, 3] = curr_cam_tran
                # Initialize Keyframe Info
                curr_keyframe = {'id': time_idx, 'est_w2c': curr_w2c, 'color': color, 'depth': depth}
                # Add to keyframe list
                keyframe_list.append(curr_keyframe)
                keyframe_time_indices.append(time_idx)
        
        # Checkpoint every iteration
        if time_idx % config["checkpoint_interval"] == 0 and config['save_checkpoints']:
            ckpt_output_dir = os.path.join(config["workdir"], config["run_name"])
            # 添加相机参数以便检查点可以用于可视化
            params_with_camera = add_camera_params_to_checkpoint(
                params, variables, intrinsics, first_frame_w2c, 
                dataset_config, gt_w2c_all_frames, keyframe_time_indices
            )
            # 保存当前 checkpoint
            save_params_ckpt(params_with_camera, ckpt_output_dir, time_idx)
            np.save(os.path.join(ckpt_output_dir, f"keyframe_time_indices{time_idx}.npy"),
                    np.array(keyframe_time_indices))

            # 只保留最近 N 个 checkpoint（例如最近 3 个）
            try:
                max_keep = 3
                # 找到目录下所有 params*.npz
                ckpt_files = []
                for fname in os.listdir(ckpt_output_dir):
                    if fname.startswith("params") and fname.endswith(".npz"):
                        # 提取中间的数字部分：paramsXXX.npz
                        num_str = fname[len("params"):-len(".npz")]
                        if num_str.isdigit():
                            ckpt_files.append((int(num_str), fname))

                # 按帧号排序
                ckpt_files.sort(key=lambda x: x[0])

                # 如果多于 max_keep 个，就删掉最早的
                if len(ckpt_files) > max_keep:
                    num_to_delete = len(ckpt_files) - max_keep
                    for i in range(num_to_delete):
                        frame_idx, params_fname = ckpt_files[i]
                        params_path = os.path.join(ckpt_output_dir, params_fname)
                        keyframe_path = os.path.join(
                            ckpt_output_dir, f"keyframe_time_indices{frame_idx}.npy"
                        )
                        try:
                            if os.path.exists(params_path):
                                os.remove(params_path)
                        except OSError:
                            pass
                        try:
                            if os.path.exists(keyframe_path):
                                os.remove(keyframe_path)
                        except OSError:
                            pass
            except Exception as e:
                print(f"[Checkpoint Cleanup Warning] Failed to clean old checkpoints: {e}")
        
        # Increment WandB Time Step
        if config['use_wandb']:
            wandb_time_step += 1

        torch.cuda.empty_cache()

    # Compute Average Runtimes
    if tracking_iter_time_count == 0:
        tracking_iter_time_count = 1
        tracking_frame_time_count = 1
    if mapping_iter_time_count == 0:
        mapping_iter_time_count = 1
        mapping_frame_time_count = 1
    tracking_iter_time_avg = tracking_iter_time_sum / tracking_iter_time_count
    tracking_frame_time_avg = tracking_frame_time_sum / tracking_frame_time_count
    mapping_iter_time_avg = mapping_iter_time_sum / mapping_iter_time_count
    mapping_frame_time_avg = mapping_frame_time_sum / mapping_frame_time_count
    print(f"\nAverage Tracking/Iteration Time: {tracking_iter_time_avg*1000} ms")
    print(f"Average Tracking/Frame Time: {tracking_frame_time_avg} s")
    print(f"Average Mapping/Iteration Time: {mapping_iter_time_avg*1000} ms")
    print(f"Average Mapping/Frame Time: {mapping_frame_time_avg} s")

    # [End-At] 打印本次运行实际停在的帧号（Case3）
    # 注意：循环最后一次的 time_idx 就是最终帧
    final_frame = min(end_frame, num_frames - 1)
    print(f"[End-At] Finished at frame {final_frame}.")
    
    # 计算最后一个已保存的检查点帧号
    # 评估应该只评估到最后一个已保存的检查点，而不是 final_frame
    # 因为 final_frame 之后的数据可能还没有保存到检查点中
    checkpoint_interval = config.get("checkpoint_interval", 100)
    last_saved_checkpoint = (final_frame // checkpoint_interval) * checkpoint_interval
    
    # 如果 final_frame 正好是检查点，那么最后一个检查点就是 final_frame
    # 否则，最后一个检查点是小于 final_frame 的最大检查点
    if final_frame % checkpoint_interval == 0:
        last_saved_checkpoint = final_frame
    else:
        # 最后一个检查点是小于 final_frame 的最大检查点
        last_saved_checkpoint = (final_frame // checkpoint_interval) * checkpoint_interval
    
    # 计算实际评估的帧数（last_saved_checkpoint 是 0-based 索引，所以需要 +1）
    eval_num_frames = last_saved_checkpoint + 1
    print(f"[Eval] Last saved checkpoint: {last_saved_checkpoint}, will evaluate frames 0 to {last_saved_checkpoint} (total: {eval_num_frames} frames)")
    
    # Save runtime statistics to file
    runtime_stats_dict = {
        "Average Tracking/Iteration Time (ms)": float(tracking_iter_time_avg * 1000),
        "Average Tracking/Frame Time (s)": float(tracking_frame_time_avg),
        "Average Mapping/Iteration Time (ms)": float(mapping_iter_time_avg * 1000),
        "Average Mapping/Frame Time (s)": float(mapping_frame_time_avg),
        "Final Frame": int(final_frame),
    }
    
    # Save as human-readable text file
    runtime_stats_txt_path = os.path.join(output_dir, "runtime_stats.txt")
    with open(runtime_stats_txt_path, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("Runtime Statistics Summary\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Average Tracking/Iteration Time: {tracking_iter_time_avg*1000:.2f} ms\n")
        f.write(f"Average Tracking/Frame Time: {tracking_frame_time_avg:.4f} s\n")
        f.write(f"Average Mapping/Iteration Time: {mapping_iter_time_avg*1000:.2f} ms\n")
        f.write(f"Average Mapping/Frame Time: {mapping_frame_time_avg:.4f} s\n")
        f.write(f"Final Frame: {final_frame}\n")
        f.write("\n" + "=" * 60 + "\n")
    print(f"\nRuntime statistics saved to: {runtime_stats_txt_path}")
    
    # Save as JSON for programmatic access
    runtime_stats_json_path = os.path.join(output_dir, "runtime_stats.json")
    with open(runtime_stats_json_path, "w", encoding="utf-8") as f:
        json.dump(runtime_stats_dict, f, indent=2)
    print(f"Runtime statistics (JSON) saved to: {runtime_stats_json_path}")
    if config['use_wandb']:
        wandb_run.log({"Final Stats/Average Tracking Iteration Time (ms)": tracking_iter_time_avg*1000,
                       "Final Stats/Average Tracking Frame Time (s)": tracking_frame_time_avg,
                       "Final Stats/Average Mapping Iteration Time (ms)": mapping_iter_time_avg*1000,
                       "Final Stats/Average Mapping Frame Time (s)": mapping_frame_time_avg,
                       "Final Stats/step": 1})
    
    # Evaluate Final Parameters
    # 使用最后一个已保存的检查点帧数进行评估，而不是 final_frame
    # 因为 final_frame 之后的数据可能还没有保存到检查点中
    print(f"Evaluating {eval_num_frames} frames (from 0 to {last_saved_checkpoint})...")
    with torch.no_grad():
        if config['use_wandb']:
            eval(dataset, params, eval_num_frames, eval_dir, sil_thres=config['mapping']['sil_thres'],
                 wandb_run=wandb_run, wandb_save_qual=config['wandb']['eval_save_qual'],
                 mapping_iters=config['mapping']['num_iters'], add_new_gaussians=config['mapping']['add_new_gaussians'],
                 eval_every=config['eval_every'])
        else:
            eval(dataset, params, eval_num_frames, eval_dir, sil_thres=config['mapping']['sil_thres'],
                 mapping_iters=config['mapping']['num_iters'], add_new_gaussians=config['mapping']['add_new_gaussians'],
                 eval_every=config['eval_every'])

    # 如果最后一帧不是检查点，保存最终检查点
    # 这样就不需要单独的 params.npz 了，所有脚本都会自动使用最新的检查点
    checkpoint_interval = config.get("checkpoint_interval", 100)
    if config['save_checkpoints'] and final_frame % checkpoint_interval != 0:
        ckpt_output_dir = os.path.join(config["workdir"], config["run_name"])
        # 添加相机参数以便检查点可以用于可视化
        params_with_camera = add_camera_params_to_checkpoint(
            params, variables, intrinsics, first_frame_w2c, 
            dataset_config, gt_w2c_all_frames, keyframe_time_indices
        )
        # 保存最终检查点
        save_params_ckpt(params_with_camera, ckpt_output_dir, final_frame)
        np.save(os.path.join(ckpt_output_dir, f"keyframe_time_indices{final_frame}.npy"),
                np.array(keyframe_time_indices))
        print(f"Saved final checkpoint: params{final_frame}.npz")

    # Close WandB Run
    if config['use_wandb']:
        wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("experiment", type=str, help="Path to experiment file")
    # [End-At] 可选结束帧参数：python scripts/splatam.py configs/replica/splatam.py --end-at 800
    parser.add_argument(
        "--end-at",
        type=int,
        default=None,
        help="Stop SLAM after reaching this frame index (inclusive). "
             "If smaller than existing checkpoint frame, nothing will be done.",
    )

    args = parser.parse_args()

    experiment = SourceFileLoader(
        os.path.basename(args.experiment), args.experiment
    ).load_module()

    # Set Experiment Seed
    seed_everything(seed=experiment.config['seed'])
    
    # 把 end-at 挂到 config 上，供 rgbd_slam 使用
    if args.end_at is not None:
        experiment.config["end_at"] = args.end_at
        print(f"[End-At] Requested end_at={args.end_at}")

    # Create Results Directory and Copy Config
    results_dir = os.path.join(
        experiment.config["workdir"], experiment.config["run_name"]
    )
    if not experiment.config['load_checkpoint']:
        os.makedirs(results_dir, exist_ok=True)
        shutil.copy(args.experiment, os.path.join(results_dir, "config.py"))

    rgbd_slam(experiment.config)